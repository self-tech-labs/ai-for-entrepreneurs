(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[65129],{7901:function(e,n,r){(window.__NEXT_P=window.__NEXT_P||[]).push(["/models/collection.zh",function(){return r(5141)}])},43677:function(e,n,r){"use strict";r.d(n,{Z:function(){return j}});var t=r(11527),a=r(50959),i=r(85274),s=r(5341);function d(e){return(0,t.jsx)("svg",{viewBox:"0 0 24 24",width:"24",height:"24",...e,children:(0,t.jsx)("path",{fill:"currentColor",d:"M4 19h6v-2H4v2zM20 5H4v2h16V5zm-3 6H4v2h13.25c1.1 0 2 .9 2 2s-.9 2-2 2H15v-2l-3 3l3 3v-2h2c2.21 0 4-1.79 4-4s-1.79-4-4-4z"})})}let l=e=>{let{children:n,className:r,...a}=e;return(0,t.jsx)("button",{className:(0,s.Z)("nextra-button nx-transition-all active:nx-opacity-50","nx-bg-primary-700/5 nx-border nx-border-black/5 nx-text-gray-600 hover:nx-text-gray-900 nx-rounded-md nx-p-1.5","dark:nx-bg-primary-300/10 dark:nx-border-white/10 dark:nx-text-gray-400 dark:hover:nx-text-gray-50",r),...a,children:n})};function o(e){return(0,t.jsx)("svg",{viewBox:"0 0 20 20",width:"1em",height:"1em",fill:"currentColor",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z",clipRule:"evenodd"})})}function c(e){return(0,t.jsxs)("svg",{width:"24",height:"24",viewBox:"0 0 24 24",fill:"none",xmlns:"http://www.w3.org/2000/svg",stroke:"currentColor",...e,children:[(0,t.jsx)("rect",{x:"9",y:"9",width:"13",height:"13",rx:"2",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round"}),(0,t.jsx)("path",{d:"M5 15H4C2.89543 15 2 14.1046 2 13V4C2 2.89543 2.89543 2 4 2H13C14.1046 2 15 2.89543 15 4V5",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round"})]})}let h=e=>{let{getValue:n,...r}=e,[i,s]=(0,a.useState)(!1);(0,a.useEffect)(()=>{if(!i)return;let e=setTimeout(()=>{s(!1)},2e3);return()=>{clearTimeout(e)}},[i]);let d=(0,a.useCallback)(async()=>{s(!0),(null==navigator?void 0:navigator.clipboard)||console.error("Access to clipboard rejected!");try{await navigator.clipboard.writeText(n())}catch(e){console.error("Failed to copy!")}},[n]);return(0,t.jsx)(l,{onClick:d,title:"Copy code",tabIndex:0,...r,children:(0,t.jsx)(i?o:c,{className:"nextra-copy-icon nx-pointer-events-none nx-h-4 nx-w-4"})})},x=e=>{let{children:n,className:r,hasCopyCode:i=!0,filename:o,...c}=e,x=(0,a.useRef)(null),g=(0,a.useCallback)(()=>{let e=document.documentElement.dataset,n="nextraWordWrap"in e;n?delete e.nextraWordWrap:e.nextraWordWrap=""},[]);return(0,t.jsxs)("div",{className:"nextra-code-block nx-relative nx-mt-6 first:nx-mt-0",children:[o&&(0,t.jsx)("div",{className:"nx-absolute nx-top-0 nx-z-[1] nx-w-full nx-truncate nx-rounded-t-xl nx-bg-primary-700/5 nx-py-2 nx-px-4 nx-text-xs nx-text-gray-700 dark:nx-bg-primary-300/10 dark:nx-text-gray-200",children:o}),(0,t.jsx)("pre",{className:(0,s.Z)("nx-bg-primary-700/5 nx-mb-4 nx-overflow-x-auto nx-rounded-xl nx-subpixel-antialiased dark:nx-bg-primary-300/10 nx-text-[.9em]","contrast-more:nx-border contrast-more:nx-border-primary-900/20 contrast-more:nx-contrast-150 contrast-more:dark:nx-border-primary-100/40",o?"nx-pt-12 nx-pb-4":"nx-py-4",r),ref:x,...c,children:a.isValidElement(n)&&"code"===n.type?n.props.children:n}),(0,t.jsxs)("div",{className:(0,s.Z)("nx-opacity-0 nx-transition [div:hover>&]:nx-opacity-100 focus-within:nx-opacity-100","nx-flex nx-gap-1 nx-absolute nx-m-[11px] nx-right-0",o?"nx-top-8":"nx-top-0"),children:[(0,t.jsx)(l,{onClick:g,className:"md:nx-hidden",title:"Toggle word wrap elvis",children:(0,t.jsx)(d,{className:"nx-pointer-events-none nx-h-4 nx-w-4"})}),i&&(0,t.jsx)(h,{getValue(){var e,n;return(null===(e=null===(n=x.current)||void 0===n?void 0:n.querySelector("code"))||void 0===e?void 0:e.textContent)||""}})]})]})},g={logo:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 206 246",fill:"none",children:[(0,t.jsx)("circle",{cx:"40",cy:"40",r:"40",fill:"currentColor"}),(0,t.jsx)("circle",{cx:"40",cy:"206",r:"40",fill:"currentColor"}),(0,t.jsx)("circle",{cx:"166",cy:"120",r:"40",fill:"currentColor"})]}),(0,t.jsx)("span",{style:{marginLeft:".4em",fontWeight:800},children:"Prompt Engineering Guide"})]}),i18n:[{locale:"en",text:"English"},{locale:"zh",text:"中文"},{locale:"jp",text:"日本語"},{locale:"pt",text:"Portugu\xeas"},{locale:"it",text:"Italian"},{locale:"tr",text:"T\xfcrk\xe7e"},{locale:"es",text:"Espa\xf1ol"},{locale:"fr",text:"Fran\xe7ais"},{locale:"kr",text:"한국어"},{locale:"ca",text:"Catal\xe0"},{locale:"fi",text:"Finnish"},{locale:"ru",text:"Русский"}],head:function(){let{title:e}=(0,i.ZR)();return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("title",{children:[e?e+" | Prompt Engineering Guide":"Prompt Engineering Guide"," "]}),(0,t.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1.0"}),(0,t.jsx)("meta",{property:"og:title",content:"Prompt Engineering Guide"}),(0,t.jsx)("meta",{property:"og:description",content:"A Comprehensive Overview of Prompt Engineering"}),(0,t.jsx)("meta",{name:"og:title",content:e?e+" | Prompt Engineering Guide":"Prompt Engineering Guide"}),(0,t.jsx)("link",{rel:"icon",href:"/144-favicon.svg",type:"image/svg+xml"}),(0,t.jsx)("link",{rel:"icon",href:"/144-favicon-dark.svg",type:"image/svg+xml",media:"(prefers-color-scheme: dark)"})]})},project:{link:"https://github.com/dair-ai/Prompt-Engineering-Guide"},chat:{link:"https://discord.gg/FUyz9vPAwf"},docsRepositoryBase:"https://github.com/dair-ai/Prompt-Engineering-Guide/tree/main/",footer:{text:"Copyright \xa9 2023 DAIR.AI"},search:{placeholder:"Search..."},components:{pre:x}};var j=g},5141:function(e,n,r){"use strict";r.r(n),r.d(n,{__toc:function(){return o}});var t=r(11527),a=r(55411),i=r(85274),s=r(43677);r(20492),r(95178);var d=r(82132),l=r(63622);let o=[{depth:2,value:"Models",id:"models"}];function c(e){let n=Object.assign({h1:"h1",p:"p",a:"a",h2:"h2",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td"},(0,d.a)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{children:"Model Collection"}),"\n","\n",(0,t.jsx)(l.UW,{emoji:"⚠️",children:(0,t.jsx)(n.p,{children:"This section is under heavy development."})}),"\n",(0,t.jsxs)(n.p,{children:["This section consists of a collection and summary of notable and foundational LLMs. (Data adopted from ",(0,t.jsx)(n.a,{href:"https://paperswithcode.com/methods/category/language-models",children:"Papers with Code"})," and the recent work by ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2303.18223.pdf",children:"Zhao et al. (2023)"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"models",children:"Models"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Release Date"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1810.04805",children:"BERT"})}),(0,t.jsx)(n.td,{children:"2018"}),(0,t.jsx)(n.td,{children:"Bidirectional Encoder Representations from Transformers"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf",children:"GPT"})}),(0,t.jsx)(n.td,{children:"2018"}),(0,t.jsx)(n.td,{children:"Improving Language Understanding by Generative Pre-Training"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1907.11692",children:"RoBERTa"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"A Robustly Optimized BERT Pretraining Approach"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",children:"GPT-2"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"Language Models are Unsupervised Multitask Learners"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1910.10683",children:"T5"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1910.13461",children:"BART"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1909.11942",children:"ALBERT"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"A Lite BERT for Self-supervised Learning of Language Representations"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1906.08237",children:"XLNet"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"Generalized Autoregressive Pretraining for Language Understanding and Generation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1909.05858",children:"CTRL"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"CTRL: A Conditional Transformer Language Model for Controllable Generation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1904.09223v1",children:"ERNIE"})}),(0,t.jsx)(n.td,{children:"2019"}),(0,t.jsx)(n.td,{children:"ERNIE: Enhanced Representation through Knowledge Integration"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2006.16668v1",children:"GShard"})}),(0,t.jsx)(n.td,{children:"2020"}),(0,t.jsx)(n.td,{children:"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2005.14165",children:"GPT-3"})}),(0,t.jsx)(n.td,{children:"2020"}),(0,t.jsx)(n.td,{children:"Language Models are Few-Shot Learners"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2201.08239v3",children:"LaMDA"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"LaMDA: Language Models for Dialog Applications"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2104.12369v1",children:"PanGu-α"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2010.11934v3",children:"mT5"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"mT5: A massively multilingual pre-trained text-to-text transformer"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2106.10715v3",children:"CPM-2"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"CPM-2: Large-scale Cost-effective Pre-trained Language Models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2110.08207",children:"T0"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Multitask Prompted Training Enables Zero-Shot Task Generalization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2109.04650",children:"HyperCLOVA"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2107.03374v2",children:"Codex"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Evaluating Large Language Models Trained on Code"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2107.02137v1",children:"ERNIE 3.0"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",children:"Jurassic-1"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Jurassic-1: Technical Details and Evaluation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2109.01652v5",children:"FLAN"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Finetuned Language Models Are Zero-Shot Learners"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2201.11990v3",children:"MT-NLG"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2110.04725v2",children:"Yuan 1.0"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2112.09332v3",children:"WebGPT"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"WebGPT: Browser-assisted question-answering with human feedback"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2112.11446v2",children:"Gopher"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2112.12731v1",children:"ERNIE 3.0 Titan"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2112.06905",children:"GLaM"})}),(0,t.jsx)(n.td,{children:"2021"}),(0,t.jsx)(n.td,{children:"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2203.02155v1",children:"InstructGPT"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Training language models to follow instructions with human feedback"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2204.06745v1",children:"GPT-NeoX-20B"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2203.07814v1",children:"AlphaCode"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Competition-Level Code Generation with AlphaCode"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2203.13474v5",children:"CodeGen"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2203.15556",children:"Chinchilla"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Shows that for a compute budget, the best performances are not achieved by the largest models but by smaller models trained on more data."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2204.07705v3",children:"Tk-Instruct"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2205.05131v3",children:"UL2"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"UL2: Unifying Language Learning Paradigms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2204.02311v5",children:"PaLM"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"PaLM: Scaling Language Modeling with Pathways"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2205.01068",children:"OPT"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"OPT: Open Pre-trained Transformer Language Models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2211.05100v3",children:"BLOOM"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.02414v1",children:"GLM-130B"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"GLM-130B: An Open Bilingual Pre-trained Model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2208.01448v2",children:"AlexaTM"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.11416v5",children:"Flan-T5"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Scaling Instruction-Finetuned Language Models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2209.14375",children:"Sparrow"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Improving alignment of dialogue agents via targeted human judgements"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.11399v2",children:"U-PaLM"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Transcending Scaling Laws with 0.1% Extra Compute"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2211.01786v1",children:"mT0"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Crosslingual Generalization through Multitask Finetuning"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2211.09085v1",children:"Galactica"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"Galactica: A Large Language Model for Science"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2212.12017v3",children:"OPT-IML"})}),(0,t.jsx)(n.td,{children:"2022"}),(0,t.jsx)(n.td,{children:"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2302.13971v1",children:"LLaMA"})}),(0,t.jsx)(n.td,{children:"2023"}),(0,t.jsx)(n.td,{children:"LLaMA: Open and Efficient Foundation Language Models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2303.08774v3",children:"GPT-4"})}),(0,t.jsx)(n.td,{children:"2023"}),(0,t.jsx)(n.td,{children:"GPT-4 Technical Report"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2303.10845v1",children:"PanGu-Σ"})}),(0,t.jsx)(n.td,{children:"2023"}),(0,t.jsx)(n.td,{children:"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2303.17564v1",children:"BloombergGPT"})}),(0,t.jsx)(n.td,{children:"2023"}),(0,t.jsx)(n.td,{children:"BloombergGPT: A Large Language Model for Finance"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://ai.google/static/documents/palm2techreport.pdf",children:"PaLM 2"})}),(0,t.jsx)(n.td,{children:"2023"}),(0,t.jsx)(n.td,{children:"A Language Model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM."})]})]})]})]})}let h={MDXContent:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,d.a)(),e.components);return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)},pageOpts:{filePath:"pages/models/collection.zh.mdx",route:"/models/collection",timestamp:1684273448e3,pageMap:[{kind:"Meta",locale:"zh",data:{index:"提示工程指南",introduction:"提示工程简介",techniques:"提示技术",applications:"提示应用",models:"模型",risks:"风险和误用",papers:"论文",tools:"工具和库",notebooks:"Prompt Engineering 笔记本",datasets:"数据集",readings:"阅读推荐",course:{title:"提示工程课程",type:"page"},services:{title:"服务",type:"page"},about:{title:"关于",type:"page"}}},{kind:"MdxPage",name:"about",route:"/about",locale:"zh"},{kind:"Folder",name:"applications",route:"/applications",children:[{kind:"Meta",locale:"zh",data:{pal:"程序辅助语言模型",generating:"生成数据",coding:"Generating Code",workplace_casestudy:"毕业生工作分类案例研究",pf:"提示函数"}},{kind:"MdxPage",name:"coding",route:"/applications/coding",locale:"zh"},{kind:"MdxPage",name:"generating",route:"/applications/generating",locale:"zh"},{kind:"MdxPage",name:"pal",route:"/applications/pal",locale:"zh"},{kind:"MdxPage",name:"pf",route:"/applications/pf",locale:"zh"},{kind:"MdxPage",name:"workplace_casestudy",route:"/applications/workplace_casestudy",locale:"zh"},{kind:"MdxPage",name:"generating_textbooks",route:"/applications/generating_textbooks",locale:"en"},{kind:"MdxPage",name:"synthetic_rag",route:"/applications/synthetic_rag",locale:"en"}]},{kind:"MdxPage",name:"applications",route:"/applications",locale:"zh"},{kind:"MdxPage",name:"course",route:"/course",locale:"zh"},{kind:"MdxPage",name:"datasets",route:"/datasets",locale:"zh"},{kind:"MdxPage",name:"index",route:"/",locale:"zh"},{kind:"Folder",name:"introduction",route:"/introduction",children:[{kind:"Meta",locale:"zh",data:{settings:"大语言模型设置",basics:"基本概念",elements:"提示词要素",tips:"设计提示的通用技巧",examples:"提示词示例"}},{kind:"MdxPage",name:"basics",route:"/introduction/basics",locale:"zh"},{kind:"MdxPage",name:"elements",route:"/introduction/elements",locale:"zh"},{kind:"MdxPage",name:"examples",route:"/introduction/examples",locale:"zh"},{kind:"MdxPage",name:"settings",route:"/introduction/settings",locale:"zh"},{kind:"MdxPage",name:"tips",route:"/introduction/tips",locale:"zh"}]},{kind:"MdxPage",name:"introduction",route:"/introduction",locale:"zh"},{kind:"Folder",name:"models",route:"/models",children:[{kind:"Meta",locale:"zh",data:{flan:"Flan",chatgpt:"ChatGPT",llama:"LLaMA","gpt-4":"GPT-4","mistral-7b":"Mistral 7B",collection:"Model Collection"}},{kind:"MdxPage",name:"chatgpt",route:"/models/chatgpt",locale:"zh"},{kind:"MdxPage",name:"collection",route:"/models/collection",locale:"zh"},{kind:"MdxPage",name:"flan",route:"/models/flan",locale:"zh"},{kind:"MdxPage",name:"gpt-4",route:"/models/gpt-4",locale:"zh"},{kind:"MdxPage",name:"llama",route:"/models/llama",locale:"zh"},{kind:"MdxPage",name:"mistral-7b",route:"/models/mistral-7b",locale:"zh"}]},{kind:"MdxPage",name:"models",route:"/models",locale:"zh"},{kind:"MdxPage",name:"notebooks",route:"/notebooks",locale:"zh"},{kind:"MdxPage",name:"papers",route:"/papers",locale:"zh"},{kind:"MdxPage",name:"readings",route:"/readings",locale:"zh"},{kind:"Folder",name:"risks",route:"/risks",children:[{kind:"Meta",locale:"zh",data:{adversarial:"对抗性提示",factuality:"真实性",biases:"偏见"}},{kind:"MdxPage",name:"adversarial",route:"/risks/adversarial",locale:"zh"},{kind:"MdxPage",name:"biases",route:"/risks/biases",locale:"zh"},{kind:"MdxPage",name:"factuality",route:"/risks/factuality",locale:"zh"}]},{kind:"MdxPage",name:"risks",route:"/risks",locale:"zh"},{kind:"MdxPage",name:"services",route:"/services",locale:"zh"},{kind:"Folder",name:"techniques",route:"/techniques",children:[{kind:"Meta",locale:"zh",data:{zeroshot:"零样本提示",fewshot:"少样本提示",cot:"链式思考（CoT）提示",consistency:"自我一致性",knowledge:"生成知识提示",tot:"思维树（ToT）",rag:"检索增强生成 (RAG)",art:"自动推理并使用工具（ART）",ape:"自动提示工程师",activeprompt:"Active-Prompt",dsp:"方向性刺激提示",react:"ReAct框架",multimodalcot:"多模态思维链提示方法",graph:"基于图的提示"}},{kind:"MdxPage",name:"activeprompt",route:"/techniques/activeprompt",locale:"zh"},{kind:"MdxPage",name:"ape",route:"/techniques/ape",locale:"zh"},{kind:"MdxPage",name:"art",route:"/techniques/art",locale:"zh"},{kind:"MdxPage",name:"consistency",route:"/techniques/consistency",locale:"zh"},{kind:"MdxPage",name:"cot",route:"/techniques/cot",locale:"zh"},{kind:"MdxPage",name:"dsp",route:"/techniques/dsp",locale:"zh"},{kind:"MdxPage",name:"fewshot",route:"/techniques/fewshot",locale:"zh"},{kind:"MdxPage",name:"graph",route:"/techniques/graph",locale:"zh"},{kind:"MdxPage",name:"knowledge",route:"/techniques/knowledge",locale:"zh"},{kind:"MdxPage",name:"multimodalcot",route:"/techniques/multimodalcot",locale:"zh"},{kind:"MdxPage",name:"rag",route:"/techniques/rag",locale:"zh"},{kind:"MdxPage",name:"react",route:"/techniques/react",locale:"zh"},{kind:"MdxPage",name:"tot",route:"/techniques/tot",locale:"zh"},{kind:"MdxPage",name:"zeroshot",route:"/techniques/zeroshot",locale:"zh"}]},{kind:"MdxPage",name:"techniques",route:"/techniques",locale:"zh"},{kind:"MdxPage",name:"tools",route:"/tools",locale:"zh"}],flexsearch:{codeblocks:!0},title:"Model Collection",headings:o},pageNextRoute:"/models/collection.zh",nextraLayout:i.ZP,themeConfig:s.Z};n.default=(0,a.j)(h)}},function(e){e.O(0,[67892,49774,92888,40179],function(){return e(e.s=7901)}),_N_E=e.O()}]);