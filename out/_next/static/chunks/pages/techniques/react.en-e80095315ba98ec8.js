(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[48021],{97717:function(e,n,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/techniques/react.en",function(){return s(58014)}])},4102:function(e,n){"use strict";n.Z={src:"/_next/static/media/react.8e7c93ae.png",height:699,width:818,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAIAAAC6O5sJAAAAl0lEQVR42jWKSw6CMBQAe/8ruXLnRhYkRgxiBPqlFEr6ea+UJrJxMqvJkODDnrJdrRBCSmmMSSnlnAlXrBc9m2gIHv+UoxA+sckoZaRzLsaIgABxdQsZ5bBYu+N+poRJr2redD4yacdXx9rN2/NChFJKgODBEaZoMzyvj8vpvbvxmVI9+OjIvGiuRf2p6m/15g0mAIwRwg+tK5tbtjPvuQAAAABJRU5ErkJggg==",blurWidth:8,blurHeight:7}},15052:function(e,n){"use strict";n.Z={src:"/_next/static/media/alfworld.da30656d.png",height:689,width:876,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAGCAIAAABxZ0isAAAAeElEQVR42jXEWwrDIBBAUfe/vUL7EzCPktiIOsZxRk20lUIuhysMHPPHP+X2GN7dS26LDjawsADgAxxInDtOhXPJ5RRMlJiZ4v9USr6usxPa+knBqEADukAucIeUBHiPGCNxa+17V2sViKjMKtdh2ad5HzsXTC75B5Yvh7jJwQoJAAAAAElFTkSuQmCC",blurWidth:8,blurHeight:6}},29113:function(e,n){"use strict";n.Z={src:"/_next/static/media/table1.e25bc12b.png",height:515,width:787,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAAAAABd+vKJAAAAMUlEQVR42g3GsREAIAgDQPZfVAtPBY+EWOlXb1pTq0+Yxr7woOmwiB96qs6GobXIcD5X7SYHcUYtegAAAABJRU5ErkJggg==",blurWidth:8,blurHeight:5}},41667:function(e,n,s){"use strict";s.d(n,{w:function(){return r}});var t=s(11527),o=s(5341),a=s(76484),i=s.n(a);function r(e){let{src:n,alt:s,full:a}=e;return(0,t.jsx)("div",{className:(0,o.Z)("mt-6 -mb-4 flex justify-center overflow-hidden rounded-xl border dark:border-zinc-800",a?"bg-white":"bg-zinc-100"),children:(0,t.jsx)(i(),{src:n,alt:s,className:(0,o.Z)("w-auto select-none bg-white",a?"":"ring-1 ring-gray-200")})})}},43677:function(e,n,s){"use strict";s.d(n,{Z:function(){return k}});var t=s(11527),o=s(50959),a=s(85274),i=s(5341);function r(e){return(0,t.jsx)("svg",{viewBox:"0 0 24 24",width:"24",height:"24",...e,children:(0,t.jsx)("path",{fill:"currentColor",d:"M4 19h6v-2H4v2zM20 5H4v2h16V5zm-3 6H4v2h13.25c1.1 0 2 .9 2 2s-.9 2-2 2H15v-2l-3 3l3 3v-2h2c2.21 0 4-1.79 4-4s-1.79-4-4-4z"})})}let l=e=>{let{children:n,className:s,...o}=e;return(0,t.jsx)("button",{className:(0,i.Z)("nextra-button nx-transition-all active:nx-opacity-50","nx-bg-primary-700/5 nx-border nx-border-black/5 nx-text-gray-600 hover:nx-text-gray-900 nx-rounded-md nx-p-1.5","dark:nx-bg-primary-300/10 dark:nx-border-white/10 dark:nx-text-gray-400 dark:hover:nx-text-gray-50",s),...o,children:n})};function c(e){return(0,t.jsx)("svg",{viewBox:"0 0 20 20",width:"1em",height:"1em",fill:"currentColor",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z",clipRule:"evenodd"})})}function h(e){return(0,t.jsxs)("svg",{width:"24",height:"24",viewBox:"0 0 24 24",fill:"none",xmlns:"http://www.w3.org/2000/svg",stroke:"currentColor",...e,children:[(0,t.jsx)("rect",{x:"9",y:"9",width:"13",height:"13",rx:"2",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round"}),(0,t.jsx)("path",{d:"M5 15H4C2.89543 15 2 14.1046 2 13V4C2 2.89543 2.89543 2 4 2H13C14.1046 2 15 2.89543 15 4V5",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round"})]})}let d=e=>{let{getValue:n,...s}=e,[a,i]=(0,o.useState)(!1);(0,o.useEffect)(()=>{if(!a)return;let e=setTimeout(()=>{i(!1)},2e3);return()=>{clearTimeout(e)}},[a]);let r=(0,o.useCallback)(async()=>{i(!0),(null==navigator?void 0:navigator.clipboard)||console.error("Access to clipboard rejected!");try{await navigator.clipboard.writeText(n())}catch(e){console.error("Failed to copy!")}},[n]);return(0,t.jsx)(l,{onClick:r,title:"Copy code",tabIndex:0,...s,children:(0,t.jsx)(a?c:h,{className:"nextra-copy-icon nx-pointer-events-none nx-h-4 nx-w-4"})})},p=e=>{let{children:n,className:s,hasCopyCode:a=!0,filename:c,...h}=e,p=(0,o.useRef)(null),x=(0,o.useCallback)(()=>{let e=document.documentElement.dataset,n="nextraWordWrap"in e;n?delete e.nextraWordWrap:e.nextraWordWrap=""},[]);return(0,t.jsxs)("div",{className:"nextra-code-block nx-relative nx-mt-6 first:nx-mt-0",children:[c&&(0,t.jsx)("div",{className:"nx-absolute nx-top-0 nx-z-[1] nx-w-full nx-truncate nx-rounded-t-xl nx-bg-primary-700/5 nx-py-2 nx-px-4 nx-text-xs nx-text-gray-700 dark:nx-bg-primary-300/10 dark:nx-text-gray-200",children:c}),(0,t.jsx)("pre",{className:(0,i.Z)("nx-bg-primary-700/5 nx-mb-4 nx-overflow-x-auto nx-rounded-xl nx-subpixel-antialiased dark:nx-bg-primary-300/10 nx-text-[.9em]","contrast-more:nx-border contrast-more:nx-border-primary-900/20 contrast-more:nx-contrast-150 contrast-more:dark:nx-border-primary-100/40",c?"nx-pt-12 nx-pb-4":"nx-py-4",s),ref:p,...h,children:o.isValidElement(n)&&"code"===n.type?n.props.children:n}),(0,t.jsxs)("div",{className:(0,i.Z)("nx-opacity-0 nx-transition [div:hover>&]:nx-opacity-100 focus-within:nx-opacity-100","nx-flex nx-gap-1 nx-absolute nx-m-[11px] nx-right-0",c?"nx-top-8":"nx-top-0"),children:[(0,t.jsx)(l,{onClick:x,className:"md:nx-hidden",title:"Toggle word wrap elvis",children:(0,t.jsx)(r,{className:"nx-pointer-events-none nx-h-4 nx-w-4"})}),a&&(0,t.jsx)(d,{getValue(){var e,n;return(null===(e=null===(n=p.current)||void 0===n?void 0:n.querySelector("code"))||void 0===e?void 0:e.textContent)||""}})]})]})},x={logo:(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 206 246",fill:"none",children:[(0,t.jsx)("circle",{cx:"40",cy:"40",r:"40",fill:"currentColor"}),(0,t.jsx)("circle",{cx:"40",cy:"206",r:"40",fill:"currentColor"}),(0,t.jsx)("circle",{cx:"166",cy:"120",r:"40",fill:"currentColor"})]}),(0,t.jsx)("span",{style:{marginLeft:".4em",fontWeight:800},children:"Prompt Engineering Guide"})]}),i18n:[{locale:"en",text:"English"},{locale:"zh",text:"中文"},{locale:"jp",text:"日本語"},{locale:"pt",text:"Portugu\xeas"},{locale:"it",text:"Italian"},{locale:"tr",text:"T\xfcrk\xe7e"},{locale:"es",text:"Espa\xf1ol"},{locale:"fr",text:"Fran\xe7ais"},{locale:"kr",text:"한국어"},{locale:"ca",text:"Catal\xe0"},{locale:"fi",text:"Finnish"},{locale:"ru",text:"Русский"}],head:function(){let{title:e}=(0,a.ZR)();return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("title",{children:[e?e+" | Prompt Engineering Guide":"Prompt Engineering Guide"," "]}),(0,t.jsx)("meta",{name:"viewport",content:"width=device-width, initial-scale=1.0"}),(0,t.jsx)("meta",{property:"og:title",content:"Prompt Engineering Guide"}),(0,t.jsx)("meta",{property:"og:description",content:"A Comprehensive Overview of Prompt Engineering"}),(0,t.jsx)("meta",{name:"og:title",content:e?e+" | Prompt Engineering Guide":"Prompt Engineering Guide"}),(0,t.jsx)("link",{rel:"icon",href:"/144-favicon.svg",type:"image/svg+xml"}),(0,t.jsx)("link",{rel:"icon",href:"/144-favicon-dark.svg",type:"image/svg+xml",media:"(prefers-color-scheme: dark)"})]})},project:{link:"https://github.com/dair-ai/Prompt-Engineering-Guide"},chat:{link:"https://discord.gg/FUyz9vPAwf"},docsRepositoryBase:"https://github.com/dair-ai/Prompt-Engineering-Guide/tree/main/",footer:{text:"Copyright \xa9 2023 DAIR.AI"},search:{placeholder:"Search..."},components:{pre:p}};var k=x},58014:function(e,n,s){"use strict";s.r(n),s.d(n,{__toc:function(){return p}});var t=s(11527),o=s(55411),a=s(85274),i=s(43677);s(20492),s(95178);var r=s(82132),l=s(41667),c=s(4102),h=s(29113),d=s(15052);let p=[{depth:2,value:"How it Works?",id:"how-it-works"},{depth:2,value:"ReAct Prompting",id:"react-prompting"},{depth:2,value:"Results on Knowledge-Intensive Tasks",id:"results-on-knowledge-intensive-tasks"},{depth:2,value:"Results on Decision Making Tasks",id:"results-on-decision-making-tasks"},{depth:2,value:"LangChain ReAct Usage",id:"langchain-react-usage"}];function x(e){let n=Object.assign({h1:"h1",p:"p",a:"a",em:"em",h2:"h2",pre:"pre",code:"code",span:"span",ul:"ul",li:"li"},(0,r.a)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{children:"ReAct Prompting"}),"\n","\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.03629",children:"Yao et al., 2022"})," introduced a framework named ReAct where LLMs are used to generate both ",(0,t.jsx)(n.em,{children:"reasoning traces"})," and ",(0,t.jsx)(n.em,{children:"task-specific actions"})," in an interleaved manner."]}),"\n",(0,t.jsx)(n.p,{children:"Generating reasoning traces allow the model to induce, track, and update action plans, and even handle exceptions. The action step allows to interface with and gather information from external sources such as knowledge bases or environments."}),"\n",(0,t.jsx)(n.p,{children:"The ReAct framework can allow LLMs to interact with external tools to retrieve additional information that leads to more reliable and factual responses."}),"\n",(0,t.jsx)(n.p,{children:"Results show that ReAct can outperform several state-of-the-art baselines on language and decision-making tasks. ReAct also leads to improved human interpretability and trustworthiness of LLMs. Overall, the authors found that best approach uses ReAct combined with chain-of-thought (CoT) that allows use of both internal knowledge and external information obtained during reasoning."}),"\n",(0,t.jsx)(n.h2,{id:"how-it-works",children:"How it Works?"}),"\n",(0,t.jsx)(n.p,{children:'ReAct is inspired by the synergies between "acting" and "reasoning" which allow humans to learn new tasks and make decisions or reasoning.'}),"\n",(0,t.jsxs)(n.p,{children:["Chain-of-thought (CoT) prompting has shown the capabilities of LLMs to carry out reasoning traces to generate answers to questions involving arithmetic and commonsense reasoning, among other tasks ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2201.11903",children:"(Wei et al., 2022)"}),". But it's lack of access to the external world or inability to update its knowledge can lead to issues like fact hallucination and error propagation."]}),"\n",(0,t.jsx)(n.p,{children:"ReAct is a general paradigm that combines reasoning and acting with LLMs. ReAct prompts LLMs to generate verbal reasoning traces and actions for a task. This allows the system to perform dynamic reasoning to create, maintain, and adjust plans for acting while also enabling interaction to external environments (e.g., Wikipedia) to incorporate additional information into the reasoning. The figure below shows an example of ReAct and the different steps involved to perform question answering."}),"\n",(0,t.jsx)(l.w,{src:c.Z,alt:"REACT"}),"\n",(0,t.jsxs)(n.p,{children:["Image Source: ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.03629",children:"Yao et al., 2022"})]}),"\n",(0,t.jsxs)(n.p,{children:["In the example above, we pass a prompt like the following question from ",(0,t.jsx)(n.a,{href:"https://hotpotqa.github.io/",children:"HotpotQA"}),":"]}),"\n",(0,t.jsx)(n.pre,{"data-language":"text","data-theme":"default",children:(0,t.jsx)(n.code,{"data-language":"text","data-theme":"default",children:(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?"})})})}),"\n",(0,t.jsxs)(n.p,{children:["Note that in-context examples are also added to the prompt but we exclude that here for simplicity. We can see that the model generates ",(0,t.jsx)(n.em,{children:"task solving trajectories"})," (Thought, Act). Obs corresponds to observation from the environment that's being interacted with (e.g., Search engine). In essence, ReAct can retrieve information to support reasoning, while reasoning helps to target what to retrieve next."]}),"\n",(0,t.jsx)(n.h2,{id:"react-prompting",children:"ReAct Prompting"}),"\n",(0,t.jsx)(n.p,{children:"To demonstrate how ReAct prompting works, let's follow an example from the paper."}),"\n",(0,t.jsx)(n.p,{children:"The first step is to select cases from a training set (e.g., HotPotQA) and compose ReAct-format trajectories. These are used as few-shot exemplars in the prompts. The trajectories consist of multiple thought-action-observation steps as shown in the figure above. The free-form thoughts are used to achieve different tasks such as decomposing questions, extracting information, performing commonsense/arithmetic reasoning, guide search formulation, and synthesizing final answer."}),"\n",(0,t.jsx)(n.p,{children:"Here is an example of what the ReAct prompt exemplars look like (obtained from the paper and shortened to one example for simplicity):"}),"\n",(0,t.jsx)(n.pre,{"data-language":"text","data-theme":"default",children:(0,t.jsxs)(n.code,{"data-language":"text","data-theme":"default",children:[(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Question What is the elevation range for the area that the eastern sector of the"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Colorado orogeny extends into?"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Thought 1 I need to search Colorado orogeny, find the area that the eastern sector"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"of the Colorado orogeny extends into, then find the elevation range of the"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"area."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Action 1 Search[Colorado orogeny]"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Colorado and surrounding areas."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Thought 2 It does not mention the eastern sector. So I need to look up eastern"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"sector."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Action 2 Lookup[eastern sector]"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Observation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"the Central Plains orogeny."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Thought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"need to search High Plains and find its elevation range."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Action 3 Search[High Plains]"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Observation 3 High Plains refers to one of two distinct land regions"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Thought 4 I need to instead search High Plains (United States)."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Action 4 Search[High Plains (United States)]"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Observation 4 The High Plains are a subregion of the Great Plains. From east to west, the"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"m).[3]"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Thought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"is 1,800 to 7,000 ft."})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"Action 5 Finish[1,800 to 7,000 ft]"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"..."})})]})}),"\n",(0,t.jsx)(n.p,{children:"Note that different prompts setups are used for different types of tasks. For tasks where reasoning is of primary importance (e.g., HotpotQA), multiple thought-action-observation steps are used for the task-solving trajectory. For decision making tasks involving lots of action steps, thoughts are used sparsely."}),"\n",(0,t.jsx)(n.h2,{id:"results-on-knowledge-intensive-tasks",children:"Results on Knowledge-Intensive Tasks"}),"\n",(0,t.jsxs)(n.p,{children:["The paper first evaluates ReAct on knowledge-intensive reasoning tasks such as question answering (HotPotQA) and fact verification (",(0,t.jsx)(n.a,{href:"https://fever.ai/resources.html",children:"Fever"}),"). PaLM-540B is used as the base model for prompting."]}),"\n",(0,t.jsx)(l.w,{src:h.Z,alt:"REACT1"}),"\n",(0,t.jsxs)(n.p,{children:["Image Source: ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.03629",children:"Yao et al., 2022"})]}),"\n",(0,t.jsx)(n.p,{children:"The prompting results on HotPotQA and Fever using different prompting methods show that ReAct generally performs better than Act (involves acting only) on both tasks."}),"\n",(0,t.jsx)(n.p,{children:"We can also observe that ReAct outperforms CoT on Fever and lags behind CoT on HotpotQA. A detailed error analysis is provided in the paper. In summary:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"CoT suffers from fact hallucination"}),"\n",(0,t.jsx)(n.li,{children:"ReAct's structural constraint reduces its flexibility in formulating reasoning steps"}),"\n",(0,t.jsx)(n.li,{children:"ReAct depends a lot on the information it's retrieving; non-informative search results derails the model reasoning and leads to difficulty in recovering and reformulating thoughts"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Prompting methods that combine and support switching between ReAct and CoT+Self-Consistency generally outperform all the other prompting methods."}),"\n",(0,t.jsx)(n.h2,{id:"results-on-decision-making-tasks",children:"Results on Decision Making Tasks"}),"\n",(0,t.jsxs)(n.p,{children:["The paper also reports results demonstrating ReAct's performance on decision making tasks. ReAct is evaluated on two benchmarks called ",(0,t.jsx)(n.a,{href:"https://alfworld.github.io/",children:"ALFWorld"})," (text-based game) and ",(0,t.jsx)(n.a,{href:"https://webshop-pnlp.github.io/",children:"WebShop"})," (online shopping website environment). Both involve complex environments that require reasoning to act and explore effectively."]}),"\n",(0,t.jsx)(n.p,{children:"Note that the ReAct prompts are designed differently for these tasks while still keeping the same core idea of combining reasoning and acting. Below is an example for an ALFWorld problem involving ReAct prompting."}),"\n",(0,t.jsx)(l.w,{src:d.Z,alt:"REACT2"}),"\n",(0,t.jsxs)(n.p,{children:["Image Source: ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2210.03629",children:"Yao et al., 2022"})]}),"\n",(0,t.jsx)(n.p,{children:"ReAct outperforms Act on both ALFWorld and Webshop. Act, without any thoughts, fails to correctly decompose goals into subgoals. Reasoning seems to be advantageous in ReAct for these types of tasks but current prompting-based methods are still far from the performance of expert humans on these tasks."}),"\n",(0,t.jsx)(n.p,{children:"Check out the paper for more detailed results."}),"\n",(0,t.jsx)(n.h2,{id:"langchain-react-usage",children:"LangChain ReAct Usage"}),"\n",(0,t.jsxs)(n.p,{children:["Below is a high-level example of how the ReAct prompting approach works in practice. We will be using OpenAI for the LLM and ",(0,t.jsx)(n.a,{href:"https://python.langchain.com/en/latest/index.html",children:"LangChain"})," as it already has built-in functionality that leverages the ReAct framework to build agents that perform tasks by combining the power of LLMs and different tools."]}),"\n",(0,t.jsx)(n.p,{children:"First, let's install and import the necessary libraries:"}),"\n",(0,t.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,t.jsxs)(n.code,{"data-language":"python","data-theme":"default",children:[(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"%%"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"capture"})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# update or install the necessary libraries"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"!pip install --upgrade openai"})}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"!pip install --upgrade langchain"})}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"!pip install --upgrade python"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"-"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"dotenv"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"!pip install google"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"-"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"search"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"-"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"results"})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:" "}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# import libraries"})}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" openai"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" os"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"from"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" langchain"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"llms "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" OpenAI"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"from"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" langchain"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"agents "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" load_tools"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"from"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" langchain"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"agents "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" initialize_agent"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"from"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" dotenv "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" load_dotenv"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"load_dotenv"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"()"})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:" "}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# load API keys; you will need to obtain these if you haven't yet"})}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"os"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"environ"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"["}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"OPENAI_API_KEY"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"]"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" os"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"getenv"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"OPENAI_API_KEY"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"os"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"environ"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"["}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"SERPER_API_KEY"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"]"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" os"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"getenv"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"SERPER_API_KEY"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:" "})]})}),"\n",(0,t.jsx)(n.p,{children:"Now we can configure the LLM, the tools we will use, and the agent that allows us to leverage the ReAct framework together with the LLM and tools. Note that we are using a search API for searching external information and LLM as a math tool."}),"\n",(0,t.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,t.jsxs)(n.code,{"data-language":"python","data-theme":"default",children:[(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"llm "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"OpenAI"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(model_name"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"text-davinci-003"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" ,temperature"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"tools "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"load_tools"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(["}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"google-serper"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:", "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"llm-math"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"], llm"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"llm)"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"agent "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"initialize_agent"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(tools, llm, agent"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"zero-shot-react-description"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:", verbose"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"True"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]})]})}),"\n",(0,t.jsx)(n.p,{children:"Once that's configured, we can now run the agent with the desired query/prompt. Notice that here we are not expected to provide few-shot exemplars as explained in the paper."}),"\n",(0,t.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,t.jsx)(n.code,{"data-language":"python","data-theme":"default",children:(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"agent"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"run"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"Who is Olivia Wilde\'s boyfriend? What is his current age raised to the 0.23 power?"'}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]})})}),"\n",(0,t.jsx)(n.p,{children:"The chain execution looks as follows:"}),"\n",(0,t.jsx)(n.pre,{"data-language":"yaml","data-theme":"default",children:(0,t.jsxs)(n.code,{"data-language":"yaml","data-theme":"default",children:[(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:">"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" Entering new AgentExecutor chain..."})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:" I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power."})}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Action"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"Search"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Action Input"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"Olivia Wilde boyfriend"'})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Observation"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"Olivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline."})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Thought"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"I need to find out Harry Styles' age."})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Action"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"Search"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Action Input"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"Harry Styles age"'})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Observation"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"29 years"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Thought"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"I need to calculate 29 raised to the 0.23 power."})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Action"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"Calculator"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Action Input"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"29^0.23"})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Observation"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Answer"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"2.169459462491557"})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:" "}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Thought"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"I now know the final answer."})]}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"Final Answer"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:"Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557."})]}),"\n",(0,t.jsx)(n.span,{className:"line",children:" "}),"\n",(0,t.jsxs)(n.span,{className:"line",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:">"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" Finished chain."})]})]})}),"\n",(0,t.jsx)(n.p,{children:"The output we get is as follows:"}),"\n",(0,t.jsx)(n.pre,{"data-language":"text","data-theme":"default",children:(0,t.jsx)(n.code,{"data-language":"text","data-theme":"default",children:(0,t.jsx)(n.span,{className:"line",children:(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:'"Harry Styles, Olivia Wilde\'s boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557."'})})})}),"\n",(0,t.jsxs)(n.p,{children:["We adapted the example from the ",(0,t.jsx)(n.a,{href:"https://python.langchain.com/docs/modules/agents/agent_types/react",children:"LangChain documentation"}),", so credit goes to them. We encourage the learner to explore different combination of tools and tasks."]}),"\n",(0,t.jsxs)(n.p,{children:["You can find the notebook for this code here: ",(0,t.jsx)(n.a,{href:"https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/react.ipynb",children:"https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/react.ipynb"})]})]})}let k={MDXContent:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,r.a)(),e.components);return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(x,{...e})}):x(e)},pageOpts:{filePath:"pages/techniques/react.en.mdx",route:"/techniques/react",timestamp:1687402124e3,pageMap:[{kind:"Meta",locale:"en",data:{index:"Prompt Engineering",introduction:"Introduction",techniques:"Techniques",applications:"Applications",models:"Models",risks:"Risks & Misuses",papers:"Papers",tools:"Tools",notebooks:"Notebooks",datasets:"Datasets",readings:"Additional Readings",course:{title:"Prompt Engineering Course",type:"page"},services:{title:"Services",type:"page"},about:{title:"About",type:"page"}}},{kind:"MdxPage",name:"about",route:"/about",locale:"en"},{kind:"Folder",name:"applications",route:"/applications",children:[{kind:"Meta",locale:"en",data:{pal:"Program-Aided Language Models",generating:"Generating Data",synthetic_rag:"Generating Synthetic Dataset for RAG",generating_textbooks:"Tackling Generated Datasets Diversity",coding:"Generating Code",workplace_casestudy:"Graduate Job Classification Case Study",pf:"Prompt Function"}},{kind:"MdxPage",name:"coding",route:"/applications/coding",locale:"en"},{kind:"MdxPage",name:"generating",route:"/applications/generating",locale:"en"},{kind:"MdxPage",name:"generating_textbooks",route:"/applications/generating_textbooks",locale:"en"},{kind:"MdxPage",name:"pal",route:"/applications/pal",locale:"en"},{kind:"MdxPage",name:"pf",route:"/applications/pf",locale:"en"},{kind:"MdxPage",name:"synthetic_rag",route:"/applications/synthetic_rag",locale:"en"},{kind:"MdxPage",name:"workplace_casestudy",route:"/applications/workplace_casestudy",locale:"en"}]},{kind:"MdxPage",name:"applications",route:"/applications",locale:"en"},{kind:"MdxPage",name:"course",route:"/course",locale:"en"},{kind:"MdxPage",name:"datasets",route:"/datasets",locale:"en"},{kind:"MdxPage",name:"index",route:"/",locale:"en"},{kind:"Folder",name:"introduction",route:"/introduction",children:[{kind:"Meta",locale:"en",data:{settings:"LLM Settings",basics:"Basics of Prompting",elements:"Prompt Elements",tips:"General Tips for Designing Prompts",examples:"Examples of Prompts"}},{kind:"MdxPage",name:"basics",route:"/introduction/basics",locale:"en"},{kind:"MdxPage",name:"elements",route:"/introduction/elements",locale:"en"},{kind:"MdxPage",name:"examples",route:"/introduction/examples",locale:"en"},{kind:"MdxPage",name:"settings",route:"/introduction/settings",locale:"en"},{kind:"MdxPage",name:"tips",route:"/introduction/tips",locale:"en"}]},{kind:"MdxPage",name:"introduction",route:"/introduction",locale:"en"},{kind:"Folder",name:"models",route:"/models",children:[{kind:"Meta",locale:"en",data:{flan:"Flan",chatgpt:"ChatGPT",llama:"LLaMA","gpt-4":"GPT-4","mistral-7b":"Mistral 7B",collection:"LLM Collection"}},{kind:"MdxPage",name:"chatgpt",route:"/models/chatgpt",locale:"en"},{kind:"MdxPage",name:"collection",route:"/models/collection",locale:"en"},{kind:"MdxPage",name:"flan",route:"/models/flan",locale:"en"},{kind:"MdxPage",name:"gpt-4",route:"/models/gpt-4",locale:"en"},{kind:"MdxPage",name:"llama",route:"/models/llama",locale:"en"},{kind:"MdxPage",name:"mistral-7b",route:"/models/mistral-7b",locale:"en"}]},{kind:"MdxPage",name:"models",route:"/models",locale:"en"},{kind:"MdxPage",name:"notebooks",route:"/notebooks",locale:"en"},{kind:"MdxPage",name:"papers",route:"/papers",locale:"en"},{kind:"MdxPage",name:"readings",route:"/readings",locale:"en"},{kind:"Folder",name:"risks",route:"/risks",children:[{kind:"Meta",locale:"en",data:{adversarial:"Adversarial Prompting",factuality:"Factuality",biases:"Biases"}},{kind:"MdxPage",name:"adversarial",route:"/risks/adversarial",locale:"en"},{kind:"MdxPage",name:"biases",route:"/risks/biases",locale:"en"},{kind:"MdxPage",name:"factuality",route:"/risks/factuality",locale:"en"}]},{kind:"MdxPage",name:"risks",route:"/risks",locale:"en"},{kind:"MdxPage",name:"services",route:"/services",locale:"en"},{kind:"Folder",name:"techniques",route:"/techniques",children:[{kind:"Meta",locale:"en",data:{zeroshot:"Zero-shot Prompting",fewshot:"Few-shot Prompting",cot:"Chain-of-Thought Prompting",consistency:"Self-Consistency",knowledge:"Generate Knowledge Prompting",tot:"Tree of Thoughts",rag:"Retrieval Augmented Generation",art:"Automatic Reasoning and Tool-use",ape:"Automatic Prompt Engineer",activeprompt:"Active-Prompt",dsp:"Directional Stimulus Prompting",react:"ReAct",multimodalcot:"Multimodal CoT",graph:"Graph Prompting"}},{kind:"MdxPage",name:"activeprompt",route:"/techniques/activeprompt",locale:"en"},{kind:"MdxPage",name:"ape",route:"/techniques/ape",locale:"en"},{kind:"MdxPage",name:"art",route:"/techniques/art",locale:"en"},{kind:"MdxPage",name:"consistency",route:"/techniques/consistency",locale:"en"},{kind:"MdxPage",name:"cot",route:"/techniques/cot",locale:"en"},{kind:"MdxPage",name:"dsp",route:"/techniques/dsp",locale:"en"},{kind:"MdxPage",name:"fewshot",route:"/techniques/fewshot",locale:"en"},{kind:"MdxPage",name:"graph",route:"/techniques/graph",locale:"en"},{kind:"MdxPage",name:"knowledge",route:"/techniques/knowledge",locale:"en"},{kind:"MdxPage",name:"multimodalcot",route:"/techniques/multimodalcot",locale:"en"},{kind:"MdxPage",name:"rag",route:"/techniques/rag",locale:"en"},{kind:"MdxPage",name:"react",route:"/techniques/react",locale:"en"},{kind:"MdxPage",name:"tot",route:"/techniques/tot",locale:"en"},{kind:"MdxPage",name:"zeroshot",route:"/techniques/zeroshot",locale:"en"}]},{kind:"MdxPage",name:"techniques",route:"/techniques",locale:"en"},{kind:"MdxPage",name:"tools",route:"/tools",locale:"en"}],flexsearch:{codeblocks:!0},title:"ReAct Prompting",headings:p},pageNextRoute:"/techniques/react.en",nextraLayout:a.ZP,themeConfig:i.Z};n.default=(0,o.j)(k)}},function(e){e.O(0,[67892,49774,92888,40179],function(){return e(e.s=97717)}),_N_E=e.O()}]);